{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc68b280-ae97-40a0-af53-cf00b6960152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749169445.913689   54301 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749169445.918276   54301 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749169445.934487   54301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749169445.934526   54301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749169445.934528   54301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749169445.934530   54301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "# suppress tensorflow logging, usually not useful unless you are having problems with tensorflow or accessing gpu\n",
    "# it seems necessary to have this environment variable set before tensorflow is imported, or else it doesn't take effect\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# imports generally useful throughout the notebook\n",
    "# usually all imports should happen at the top of a notebook, but in\n",
    "# these notebooks where the purpose is to show how to use the Keras API\n",
    "# the relevant imports will happen in the cells where the API is discussed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# global settings for notebook output and images\n",
    "plt.rcParams['figure.figsize'] = (8, 8) # set default figure size, 10in by 8in\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6f1eb3-9a86-4504-84e5-8b6ab0fc13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import project defined modules / functions used in this notebook\n",
    "# ensure that the src directory where project modules are found is on\n",
    "# the PYTHONPATH\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# assignment function imports for doctests and github autograding\n",
    "# these are required for assignment autograding\n",
    "from nndl import vectorize_samples, plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379b6651-8fc8-45d1-bb7a-6862549b2c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices :  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Available Devices :  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# if want to restrict to cpu or gpu, configure visible device for rest of notebook to use\n",
    "dev = tf.config.list_physical_devices()\n",
    "print('Physical Devices : ', dev)\n",
    "\n",
    "#tf.config.set_visible_devices(dev[0])\n",
    "#tf.config.set_visible_devices(dev[1])\n",
    "#dev = tf.config.list_logical_devices()\n",
    "print('Available Devices : ', dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd77286-a453-45d8-92f2-b57fddeab8f0",
   "metadata": {},
   "source": [
    "# Chapter 7: Working with Keras: A deep dive\n",
    "\n",
    "Supporting materials for:\n",
    "\n",
    "Chollet (2021). *Deep Learning with Python*. 2nd ed. Manning Publications Co.\n",
    "[Amazon](https://www.amazon.com/Learning-Python-Second-Fran%C3%A7ois-Chollet/dp/1617296864/ref=sr_1_1?crid=32NFM2SBCJVQQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d51b87-3d51-46e6-96f3-c909014ab38d",
   "metadata": {},
   "source": [
    "After the three practical examples from the previous unit, you should be starting to feel\n",
    "familiar with how to approach classification and regression problems using neural networks.\n",
    "\n",
    "You've also experienced some discussion on the central problem of machine learning: **overfitting**.\n",
    "\n",
    "In this unit, we are going to take a more detailed look at the Keras API.  A better understanding of some\n",
    "of the details of the different `keras` workflows will help you to better understand the examples\n",
    "and models we will develop in upcoming chapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8db353-eff6-43e1-9286-98f3b31cc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reuse function from section 7.3 below\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# create a model, separate function for reuse, example\n",
    "# here again of using Functional API to create the model, though\n",
    "# it is a simple sequential multi-class classification with a dense\n",
    "# layer and some dropout regularization\n",
    "def get_mnist_model():\n",
    "    inputs = keras.Input(shape=(28 * 28,))\n",
    "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(0.5)(features)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# load and normalize the mnist data, reserviing some of the 60000\n",
    "# in train set for validation\n",
    "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
    "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
    "train_images, val_images = images[10000:], images[:10000]\n",
    "train_labels, val_labels = labels[10000:], labels[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4c249-6d00-4282-acd7-41c17da47c3c",
   "metadata": {},
   "source": [
    "## 7.4 Writing your own Training and Evaluation loops\n",
    "\n",
    "As a reminder, the contents of a typical training loop for supervised learning look like this:\n",
    "\n",
    "1. Run the forward pass (compute the model’s output) inside a gradient tape to\n",
    "   obtain a loss value for the current batch of data.\n",
    "2. Retrieve the gradients of the loss with regard to the model’s weights.\n",
    "3. Update the model’s weights so as to lower the loss value on the current batch\n",
    "   of data.\n",
    "\n",
    "These steps are repeated for as many batches as neessary.  This is essentially what the `fit()`\n",
    "method does under the hood.\n",
    "\n",
    "In this section we look at reimplementing `fit()` from scratch, so that if needed you can add\n",
    "bells and whistles or implement any training algorithm you may need to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f024bf-863f-4cd0-9508-be64da61deed",
   "metadata": {},
   "source": [
    "### 7.4.1 Training versus inference\n",
    "\n",
    "Some Keras layers, such as the `Dropout` layer that you saw, have different behaviors during *training* and during\n",
    "*inference*.  For such layers you need to set the `training=True` argument of the `call()` function when performing\n",
    "the forward pass, and `training=False` during inference.  Thus generically the forward pass if we are writing it\n",
    "by hand is usually something like\n",
    "\n",
    "```python\n",
    "predictions = model(inputs, training=True)\n",
    "```\n",
    "\n",
    "In addition when retrieving the gradients to perform the backward pass you should use:\n",
    "\n",
    "```python\n",
    "tape.gradients(loss, model.trainable_weights)\n",
    "```\n",
    "\n",
    "because layers and models own two kinds of weights:\n",
    "\n",
    "- **Trainable weights** These are meant to be updated via backpropagation to minimize\n",
    "  the loss of the model, such as the `Dense` layers weights and biases.\n",
    "- **Non-trainable weights** These are meant to be updated during the forward pass\n",
    "  by the layers that own them.\n",
    "\n",
    "So given these ideas, a supervised-learning step in Keras ends up looking like the following:\n",
    "\n",
    "```python\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "        gradients = tape.gradients(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(model.trainable_weights, gradients))\n",
    "```\n",
    "\n",
    "### 7.4.2 Low-level usage of metrics\n",
    "\n",
    "In a low-level training loop, you will probably want to leverage Keras metrics. \n",
    "To use a metric by hand, simply call `update_state(y_true, y_pred)` on the metric object\n",
    "for each batch of targets and predictions, and then use `result()` to query the current metric value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d447a971-1111-4724-81bc-533370860fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749169911.405308   54301 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9706 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "targets = [0, 1, 2]\n",
    "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "metric.update_state(targets, predictions)\n",
    "current_result = metric.result()\n",
    "print(f\"result: {current_result:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d670fe-b899-479f-98e6-8a3a0a3e58b5",
   "metadata": {},
   "source": [
    "You may also need to track the average of a scalar value, such as the model's loss.  You\n",
    "can do this using `keras.metrics.Mean` metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "414f359a-e40f-47bf-8bdd-ae9215cc83b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of values: 2.00\n"
     ]
    }
   ],
   "source": [
    "values = [0, 1, 2, 3, 4]\n",
    "mean_tracker = keras.metrics.Mean()\n",
    "for value in values:\n",
    "    mean_tracker.update_state(value)\n",
    "print(f\"Mean of values: {mean_tracker.result():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb24d56-de16-40aa-bd1c-568caaf0af3c",
   "metadata": {},
   "source": [
    "Remember to use `metric.reset_state()` when you want to reset the current results at the start of a training\n",
    "epoch or at the start of evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cca4d4-c454-4e53-a267-867cacd1efb1",
   "metadata": {},
   "source": [
    "### 7.4.3 A complete training and evaluation loop\n",
    "\n",
    "As an example, lets combine the forward pass, backward pass and metrics tracking into a `fit()` like\n",
    "training step function that takes a batch of data and targets and returns the logs that\n",
    "would get displayed by the `fit()` progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81dae715-0640-42a5-836a-b040edff9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mnist_model()\n",
    "\n",
    "# prepare the loss function, optimizer and list of metrics to monitor\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.RMSprop()\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "# use a metrice.mean to track the loss average\n",
    "loss_tracking_metric = keras.metrics.Mean()\n",
    "\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "    # keep track of metrics\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[metric.name] = metric.result()\n",
    "\n",
    "    # keep track of the loss average\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"loss\"] = loss_tracking_metric.result()\n",
    "\n",
    "    # return the current values of the metrics and loss\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26a901-1eee-45b5-84c3-919c2768e742",
   "metadata": {},
   "source": [
    "We need to reset the state of our metrics at the start of each epoch (the above function is called to update\n",
    "for each batch created during an epoch of training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb05e182-618c-460d-92d2-c338118c17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_metrics():\n",
    "    for metric in metrics:\n",
    "        metric.reset_state()\n",
    "        loss_tracking_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b5a40-10fa-4f82-8c17-5a1d4ce9738d",
   "metadata": {},
   "source": [
    "We can now lay out our complete training loop.  Not that we use a `tf.data.Dataset` object to turn\n",
    "our NumPy data into an iterator that iterates over the data in batches of size 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9156128-f692-4202-8fd6-a741b494893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at the end of epoch 0\n",
      "...sparse_categorical_accuracy: 0.9136\n",
      "...loss: 0.2886\n",
      "Results at the end of epoch 1\n",
      "...sparse_categorical_accuracy: 0.9540\n",
      "...loss: 0.1623\n",
      "Results at the end of epoch 2\n",
      "...sparse_categorical_accuracy: 0.9639\n",
      "...loss: 0.1291\n"
     ]
    }
   ],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_images, train_labels))\n",
    "training_dataset = training_dataset.batch(32)\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    reset_metrics()\n",
    "    for inputs_batch, targets_batch in training_dataset:\n",
    "        logs = train_step(inputs_batch, targets_batch)\n",
    "    print(f\"Results at the end of epoch {epoch}\")\n",
    "    for key, value in logs.items():\n",
    "        print(f\"...{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c732c-c73c-4a31-90a6-c5b865a3f897",
   "metadata": {},
   "source": [
    "And to complete the by hand example, similarly we need an evaluation loop.  A simple loop that repeatedly\n",
    "calls a `test_step()` function, which processes a single batch of data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9395354-e132-4cd5-92b4-a5cec913938c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "...val_sparse_categorical_accuracy: 0.9666\n",
      "...val_loss: 0.1241\n"
     ]
    }
   ],
   "source": [
    "# this is really a subset of the train_step, just omitting the\n",
    "# calculation of gradients and weight update.  Notice that\n",
    "# training=false\n",
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_fn(targets, predictions)\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[\"val_\" + metric.name] = metric.result()\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
    "    return logs\n",
    "\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "reset_metrics()\n",
    "\n",
    "for inputs_batch, targets_batch in val_dataset:\n",
    "    logs = test_step(inputs_batch, targets_batch)\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in logs.items():\n",
    "    print(f\"...{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686f9a4-3f11-4e1d-aa9c-9eb7734e47cb",
   "metadata": {},
   "source": [
    "You may notice that performance is pretty bad for the previous examples.  `fit()` and `evaluate()` support many more features,\n",
    "including large-scale distributed computation, which requires a bit more work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64174d6d-c508-4301-b884-d1e330e642d1",
   "metadata": {},
   "source": [
    "### 7.4.4 Make it fast with tf.function\n",
    "\n",
    "You may have noticed that your custom loops are running significantly slower than the\n",
    "built-in fit() and evaluate(), despite implementing essentially the same logic.\n",
    "That’s because, by default, TensorFlow code is executed line by line, *eagerly*,\n",
    "e.g. it is interpreted.\n",
    "\n",
    "It is more performant to **compile** your TensorFlow code into a\n",
    "**computation graph** that can be globally optimized in a way that code \n",
    "interpreted line by line cannot.  Luckily we can simply add the `@tf.function`\n",
    "decorator to any function to indicate this should be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a81a4596-5ba1-4c45-a9df-2c57cb9905bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at the end of epoch 0\n",
      "...sparse_categorical_accuracy: 0.9757\n",
      "...loss: 0.0910\n",
      "Results at the end of epoch 1\n",
      "...sparse_categorical_accuracy: 0.9780\n",
      "...loss: 0.0824\n",
      "Results at the end of epoch 2\n",
      "...sparse_categorical_accuracy: 0.9794\n",
      "...loss: 0.0781\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "    # keep track of metrics\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[metric.name] = metric.result()\n",
    "\n",
    "    # keep track of the loss average\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"loss\"] = loss_tracking_metric.result()\n",
    "\n",
    "    # return the current values of the metrics and loss\n",
    "    return logs\n",
    "    \n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "reset_metrics()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    reset_metrics()\n",
    "    for inputs_batch, targets_batch in training_dataset:\n",
    "        logs = train_step(inputs_batch, targets_batch)\n",
    "    print(f\"Results at the end of epoch {epoch}\")\n",
    "    for key, value in logs.items():\n",
    "        print(f\"...{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42dbdc-60a3-45fa-9fb0-022a3dfca780",
   "metadata": {},
   "source": [
    "On my system, this goes from taking many seconds to run, to returning almost immediately\n",
    "when add in the compile decorator.\n",
    "\n",
    "Remember, while you are debugging your code, prefer running it eagerly, without\n",
    "any @tf.function decorator. It’s easier to track bugs this way. Once your code is working\n",
    "and you want to make it fast, add a @tf.function decorator to your training step\n",
    "and your evaluation step—or any other performance-critical function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb371f-f6f7-45c4-9738-48b22abd4ed5",
   "metadata": {},
   "source": [
    "### 7.4.5 Leveraging `fit()` with a custom training loop\n",
    "\n",
    "What if you need a custom training algorithm, but you still want to leverage the\n",
    "power of the built-in Keras training logic? There’s actually a middle ground between\n",
    "fit() and a training loop written from scratch: you can provide a custom training\n",
    "step function and let the framework do the rest.\n",
    "\n",
    "Here is a simple example\n",
    "\n",
    "- We create a new class that subclasses keras.Model.\n",
    "- We override the method train_step(self, data).\n",
    "- We implement a metrics property that tracks the model’s Metric instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bd1ba31-0a97-42f9-8318-d0867d130ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss_tracker will be used to track the average of per-batch losses during training\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "# our own Model so can override train_step\n",
    "class CustomModel(keras.Model):\n",
    "\n",
    "    # override the train_step method, this is basically the same as was developed\n",
    "    # in previous example\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            # but using self(inputs, training=True) instead of model() since our\n",
    "            # model is this class instance itself\n",
    "            predictions = self(inputs, training=True)\n",
    "            loss = loss_fn(targets, predictions)\n",
    "\n",
    "        # NOTE: typo in textbook, these need to be self.trainable_weights(),\n",
    "        # and self.optimizer.apply_gradients()\n",
    "        # using model.trainable_weights() is referring to an external variable\n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "\n",
    "        # update the loss tracker metric that tracks the average of the loss\n",
    "        loss_tracker.update_state(loss)\n",
    "        # return the average loss so far by querying the loss tracker metric\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    # any metric you would like to reset across epochs should be listed here\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [loss_tracker]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7423a-e251-41c5-b0d7-3412a38f3ece",
   "metadata": {},
   "source": [
    "We can now instantiate our custom model, compile it, and train it using `fit()` as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a04a38df-baea-4219-8e43-58614a8f214d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749170920.788243   55664 service.cc:152] XLA service 0x7cdd480225d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1749170920.788284   55664 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "I0000 00:00:1749170920.913773   55664 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  68/1563\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.2701"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749170922.369365   55664 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.4455\n",
      "Epoch 2/3\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.1641\n",
      "Epoch 3/3\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7cde39541160>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo getting \"numpy() is only available when eager execution is enabled.\", though using TensorFlow 2.x and it appears enabled\n",
    "# need to debug this example further.\n",
    "#import tensorflow as tf\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "inputs = keras.Input(shape=(28 * 28,))\n",
    "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "features = layers.Dropout(0.5)(features)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = CustomModel(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop())\n",
    "model.fit(train_images, train_labels, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074b0b5f-55f4-4274-a0da-7684f1b5354b",
   "metadata": {},
   "source": [
    "A couple of points to note:\n",
    "\n",
    "- This pattern does not prevent you from building models with the Functional\n",
    "  API. You can do this whether you’re building Sequential models, Functional\n",
    "   API models, or subclassed models.\n",
    "- You don’t need to use a @tf.function decorator when you override train_\n",
    "  step—the framework does it for you.\n",
    "\n",
    "Now what about metrics and what about configuring the loss via `compile()?\n",
    "\n",
    "After calling `compile()` you get access to the following:\n",
    "\n",
    "- **self.compiled_loss** The loss function you passed to compile().\n",
    "- **self.compiled_metrics** A wrapper for the list of metrics you passed, which\n",
    "  allows you to call self.compiled_metrics.update_state() to update all of\n",
    "  your metrics at once.\n",
    "- **self.metrics** The actual list of metrics you passed to compile(). Note that it\n",
    "  also includes a metric that tracks the loss, similar to what we did manually with\n",
    "  our loss_tracking_metric earlier.\n",
    "\n",
    "We can thus modify to more tightly integreate with the Keras workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef1b68-6444-412e-b632-c4a3275cd9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs, training=True)\n",
    "            # compute the loss via the one given to the compile() method\n",
    "            loss = self.compiled_loss(targets, predictions)\n",
    "            \n",
    "        # same note as before, need to be self.trainable_weights and self.compiled_metrics,\n",
    "        # looks like bug in book\n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "        \n",
    "        # likewise use the metrics from compile() and update\n",
    "        self.compiled_metrics.update_state(targets, predictions)\n",
    "            \n",
    "        # return a dict mapping metric names to their current values\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed1bd5-da43-4e77-aaf1-642a4dacc9ac",
   "metadata": {},
   "source": [
    "Let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07d731f7-9b4d-4007-9add-f26b43ea4605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.4491\n",
      "Epoch 2/3\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.1629\n",
      "Epoch 3/3\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.1333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7cde38997aa0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo getting \"numpy() is only available when eager execution is enabled.\", though using TensorFlow 2.x and it appears enabled\n",
    "# need to debug this example further.\n",
    "inputs = keras.Input(shape=(28 * 28,))\n",
    "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "features = layers.Dropout(0.5)(features)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = CustomModel(inputs, outputs)\n",
    "\n",
    "# a more normal looking invocation of compile, give the optimizer, loss\n",
    "# and metrics \n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ca199-ae25-4a99-ab33-1ac9fc2a0152",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Some of the important things to keep in mind about writing your own training and evaluation loops:\n",
    "\n",
    "<font color='blue'>\n",
    "\n",
    "- The contents of a typical training loop is to run the forward pass on a batch inside a gradient tape to obtain loss, retrieve the gradients of\n",
    "  the loss with respect to the model's weights, update the weights to lower the loss on current batch of data.\n",
    "- Layers and models have trainable and non-trainable weights\n",
    "- You need to *compile* your TensorFlow code into a **computation graph** after debugging it in order to get expected performance.  Use `@tf.function` decorator.\n",
    "- A good middle ground if you need something special in your training loop is to provide a custom training step function and let the framework do the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950e654-45c5-4f3e-be00-0a6b67763902",
   "metadata": {},
   "source": [
    "# Chapter Summary\n",
    "\n",
    "Things to remember about advanced features of working with the Keras API\n",
    "\n",
    "<font color='blue'>\n",
    "\n",
    "- Keras offers a spectrum of different workflows.  They all should smoothly inter-operate together.\n",
    "- You can build models via the `Sequential` class, via the Functional API, or by\n",
    "  subclassing the `Model` class.  Most of the time you'll be using the Function API as it gives a\n",
    "  good balance between convenience and ability to modify for specific needs.\n",
    "- The simplest way to train and evaluate a model is via the default fit() and\n",
    "  evaluate() methods.\n",
    "- Keras callbacks provide a simple way to monitor models during your call to\n",
    "  fit() and automatically take action based on the state of the model.\n",
    "- You can also fully take control of what fit() does by overriding the train_\n",
    "  step() method.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
