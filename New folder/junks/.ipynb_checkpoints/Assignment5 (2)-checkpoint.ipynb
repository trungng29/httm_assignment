{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea8ad48-12f0-4501-a8ac-1a59430a17e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5.1: Overfitting Demonstration\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Build a model prone to overfitting (too complex for the data)\n",
    "overfit_model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(2,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "overfit_model.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history_overfit = overfit_model.fit(X_train, y_train,\n",
    "                                    epochs=100,\n",
    "                                    batch_size=32,\n",
    "                                    validation_data=(X_test, y_test),\n",
    "                                    verbose=0) # verbose=0 for cleaner output\n",
    "\n",
    "# 3. Plotting the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history_overfit.history['loss'], label='Training Loss')\n",
    "plt.plot(history_overfit.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Overfitting Model - Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26376c15-e2ee-4487-b185-9b4092d9d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5.1: Preventing Overfitting\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Use the same data as before\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Build a regularized model\n",
    "regularized_model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', \n",
    "                 kernel_regularizer=regularizers.l2(0.001), # L2 Regularization\n",
    "                 input_shape=(2,)),\n",
    "    layers.Dropout(0.5), # Dropout Layer\n",
    "    layers.Dense(256, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.001)), # L2 Regularization\n",
    "    layers.Dropout(0.5), # Dropout Layer\n",
    "    layers.Dense(128, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.001)), # L2 Regularization\n",
    "    layers.Dropout(0.5), # Dropout Layer\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "regularized_model.compile(optimizer='adam',\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "history_regularized = regularized_model.fit(X_train, y_train,\n",
    "                                            epochs=100,\n",
    "                                            batch_size=32,\n",
    "                                            validation_data=(X_test, y_test),\n",
    "                                            verbose=0)\n",
    "\n",
    "# 3. Plotting the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history_regularized.history['loss'], label='Training Loss')\n",
    "plt.plot(history_regularized.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Regularized Model - Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7f741-dd7b-4964-ab4e-1b7363098457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5.2: Setup from Assignment 4.2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load and Prepare the Data\n",
    "FILE_PATH = r\"C:\\DATA\\data_4.2.csv\"\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# Feature Engineering: Create BMI and BMI_Category\n",
    "df['bmi'] = df['weight_kg'] / ((df['height_cm'] / 100) ** 2)\n",
    "def categorize_bmi(bmi):\n",
    "    if bmi < 18.5: return 'Underweight'\n",
    "    elif 18.5 <= bmi < 25: return 'Normal'\n",
    "    else: return 'Overweight'\n",
    "df['BMI_Category'] = df['bmi'].apply(categorize_bmi)\n",
    "\n",
    "# 2. Preprocessing for Neural Networks\n",
    "features_to_drop = ['id', 'name', 'height_cm', 'weight_kg', 'bmi', 'BMI_Category']\n",
    "X = df.drop(columns=features_to_drop)\n",
    "y = df['BMI_Category']\n",
    "y_encoded = pd.get_dummies(y).values\n",
    "\n",
    "# Identify numerical and categorical features for preprocessing\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split and process data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "X_train_processed = preprocessor.fit_transform(X_train).toarray()\n",
    "X_test_processed = preprocessor.transform(X_test).toarray()\n",
    "\n",
    "# Get the number of features after one-hot encoding\n",
    "n_features = X_train_processed.shape[1]\n",
    "n_classes = y_train.shape[1]\n",
    "\n",
    "# Reshape Data for CNN\n",
    "X_train_cnn = X_train_processed.reshape((X_train_processed.shape[0], n_features, 1))\n",
    "X_test_cnn = X_test_processed.reshape((X_test_processed.shape[0], n_features, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085f478-2bc0-43bb-9cc9-c68d79a506f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5.2: Inducing Overfitting\n",
    "\n",
    "# Define the CNN model structure from Assignment 4.2\n",
    "def build_cnn_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train for more epochs to induce overfitting\n",
    "cnn_overfit = build_cnn_model((n_features, 1))\n",
    "history_cnn_overfit = cnn_overfit.fit(X_train_cnn, y_train,\n",
    "                                      epochs=50,\n",
    "                                      batch_size=64,\n",
    "                                      validation_data=(X_test_cnn, y_test),\n",
    "                                      verbose=0)\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history_cnn_overfit.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_cnn_overfit.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('CNN Model Overfitting - Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e208c6-8c0d-4dc0-8882-4ea292784d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5.2: Applying Learning Rate Scheduler\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Define the learning rate scheduler callback\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', # metric to watch\n",
    "                                 factor=0.2,       # factor to reduce LR by\n",
    "                                 patience=3,       # epochs to wait for improvement\n",
    "                                 min_lr=0.00001)   # minimum learning rate\n",
    "\n",
    "cnn_lr_tuned = build_cnn_model((n_features, 1))\n",
    "\n",
    "print(\"--- Training CNN with Learning Rate Scheduler (by Nguyễn Đức Khải) ---\")\n",
    "history_cnn_lr = cnn_lr_tuned.fit(X_train_cnn, y_train,\n",
    "                                  epochs=50,\n",
    "                                  batch_size=64,\n",
    "                                  validation_data=(X_test_cnn, y_test),\n",
    "                                  callbacks=[lr_scheduler], # Add the callback here\n",
    "                                  verbose=0)\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history_cnn_lr.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_cnn_lr.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('CNN with Learning Rate Scheduler - Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9136a7a-ff9e-4722-81d9-29c51a27e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5.2: Applying Data Augmentation\n",
    "\n",
    "# Create a function to add noise\n",
    "def augment_data(X_data, noise_level=0.05):\n",
    "    # Find the columns that are numerical (before one-hot encoding)\n",
    "    num_feature_indices = [X.columns.get_loc(c) for c in numerical_features]\n",
    "    \n",
    "    X_augmented = X_data.copy()\n",
    "    # Generate noise with the same shape as the numerical part\n",
    "    noise = np.random.normal(0, noise_level, size=(X_data.shape[0], len(num_feature_indices)))\n",
    "    \n",
    "    # We need to know which columns in the PROCESSED data correspond to numerical features.\n",
    "    # Since StandardScaler was used, they are the first columns.\n",
    "    num_cols_processed = len(num_feature_indices)\n",
    "    X_augmented[:, :num_cols_processed] += noise\n",
    "    return X_augmented\n",
    "\n",
    "# Augment the training data\n",
    "X_train_augmented = augment_data(X_train_processed)\n",
    "X_train_cnn_aug = X_train_augmented.reshape((X_train_augmented.shape[0], n_features, 1))\n",
    "\n",
    "# Build and train a new model on the augmented data\n",
    "cnn_augmented = build_cnn_model((n_features, 1))\n",
    "\n",
    "history_cnn_aug = cnn_augmented.fit(X_train_cnn_aug, y_train,\n",
    "                                    epochs=50,\n",
    "                                    batch_size=64,\n",
    "                                    validation_data=(X_test_cnn, y_test),\n",
    "                                    verbose=0)\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history_cnn_aug.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_cnn_aug.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('CNN with Data Augmentation - Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242ee4f-f804-4163-88fb-a8763c9b9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5.4.a: Load Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "# Please make sure this file is available on your computer\n",
    "FILE_PATH_REVIEW = r\"C:\\DATA\\filmReview_1.csv\" \n",
    "\n",
    "try:\n",
    "    df_reviews = pd.read_csv(FILE_PATH_REVIEW)\n",
    "    print(\"--- Dataset 'filmReview_1.csv' loaded ---\")\n",
    "    print(\"\\nDataset Info:\")\n",
    "    df_reviews.info()\n",
    "    \n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    print(df_reviews.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file was not found at {FILE_PATH_REVIEW}\")\n",
    "    print(\"Please download the 'filmReview_1.csv' file and place it in the C:\\\\DATA directory.\")\n",
    "    # Create a dummy dataframe to allow the rest of the code to run for demonstration\n",
    "    data = {\n",
    "        'review_id': range(5),\n",
    "        'user_id': ['user_001']*5,\n",
    "        'user_name': ['user1']*5,\n",
    "        'film_title': ['Movie A', 'Movie B', 'Movie C', 'Movie D', 'Movie E'],\n",
    "        'rating': [8, 2, 9, 4, 7],\n",
    "        'review_text': [\n",
    "            'This movie was absolutely fantastic! The acting was superb.',\n",
    "            'A complete waste of time. The plot was predictable and boring.',\n",
    "            'A masterpiece of cinema. I was captivated from start to finish.',\n",
    "            'It was an okay movie, nothing special but not terrible either.',\n",
    "            'I enjoyed it a lot. The characters were very well developed.'\n",
    "        ],\n",
    "        'review_date': ['2023-01-01']*5\n",
    "    }\n",
    "    df_reviews = pd.DataFrame(data)\n",
    "    print(\"\\n--- Created a dummy dataframe for demonstration purposes. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa9ecc-4bdc-4dfd-b68a-55ddc80406f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5.4.b: Word Vector Representation using spaCy\n",
    "\n",
    "# You need to install spaCy and its model first:\n",
    "# 1. pip install spacy\n",
    "# 2. python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"--- spaCy model 'en_core_web_sm' loaded ---\")\n",
    "    \n",
    "    # spaCy's .vector attribute gives an average vector for the whole text\n",
    "    # This is much more convenient than averaging word vectors manually.\n",
    "    \n",
    "    # Demonstrate on a sample review\n",
    "    sample_review = df_reviews['review_text'].iloc[0]\n",
    "    \n",
    "    # Process the text with the spaCy pipeline\n",
    "    doc = nlp(sample_review)\n",
    "    \n",
    "    # Get the sentence vector\n",
    "    sample_vector_spacy = doc.vector\n",
    "    \n",
    "    print(f\"\\nSample Review: '{sample_review}'\")\n",
    "    print(f\"Shape of spaCy vector: {sample_vector_spacy.shape}\")\n",
    "    print(f\"First 10 dimensions of the vector:\\n{sample_vector_spacy[:10]}\")\n",
    "\n",
    "except OSError:\n",
    "    print(\"\\nERROR: spaCy model 'en_core_web_sm' not found.\")\n",
    "    print(\"Please run this command in your terminal or command prompt:\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1307b47-2b40-4296-bb2d-7b47c8f16f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5.4.c&d: Keras Data Preparation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df_reviews['review_text']\n",
    "y = df_reviews['rating'].values\n",
    "\n",
    "# Normalize the target 'rating' to be between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "vocab_size = 10000  # Consider only the top 10,000 words\n",
    "oov_token = \"<OOV>\" # Token for out-of-vocabulary words\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert texts to sequences of integers\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "max_length = 100 # Max length of a review\n",
    "padding_type = 'post'\n",
    "truncation_type = 'post'\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding=padding_type, truncating=truncation_type)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding=padding_type, truncating=truncation_type)\n",
    "\n",
    "print(f\"Shape of padded training data: {X_train_pad.shape}\")\n",
    "print(f\"Shape of training labels: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c8306-3d87-4814-84d2-30aacaf8dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5.4.c&d: LSTM Model with Overfitting\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 64\n",
    "\n",
    "# Build the model\n",
    "overfit_lstm = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid') # Sigmoid for output between 0 and 1\n",
    "])\n",
    "\n",
    "# Compile the model for regression\n",
    "overfit_lstm.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "overfit_lstm.summary()\n",
    "\n",
    "print(\"\\n--- Training Overfitting LSTM Model ---\")\n",
    "history_overfit = overfit_lstm.fit(X_train_pad, y_train,\n",
    "                                   epochs=20,\n",
    "                                   validation_data=(X_test_pad, y_test),\n",
    "                                   verbose=2)\n",
    "\n",
    "# Plotting the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_overfit.history['loss'], label='Training Loss')\n",
    "plt.plot(history_overfit.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Overfitting LSTM Model - Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7713206-317f-48fc-a3e4-01475c4debeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 5.4.d: Improved LSTM Model\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the improved model with Dropout\n",
    "improved_lstm = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)), # Return sequences for next LSTM layer\n",
    "    Dropout(0.4), # Add dropout\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.4), # Add dropout\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.4), # Add dropout\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "improved_lstm.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Define Early Stopping\n",
    "# This will stop training when the validation loss has not improved for 3 epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "print(\"--- Training Improved LSTM Model with Dropout and Early Stopping ---\")\n",
    "history_improved = improved_lstm.fit(X_train_pad, y_train,\n",
    "                                     epochs=20, # We can set a high number, early stopping will handle it\n",
    "                                     validation_data=(X_test_pad, y_test),\n",
    "                                     callbacks=[early_stopping],\n",
    "                                     verbose=2)\n",
    "\n",
    "# Plotting the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_improved.history['loss'], label='Training Loss')\n",
    "plt.plot(history_improved.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Improved LSTM Model - Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b7fdf-cbb6-40d8-b2e5-c86c0c497ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbe539-af16-4c40-9039-d6a5b9a79698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05118b-0ca8-4218-97ab-1872e212269b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad0de0-1cdd-4212-8ceb-b8971b2fd63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
